整理自AWS Blog https://aws.amazon.com/blogs/database/key-components-of-a-data-driven-agentic-ai-application/

Agentic AI能够提升生产效率，解决传统软件不能解决的问题，简化集成，找到之前不可见的答案。 Agentic AI系统自动决定如何以及调用何种action（动作）来完成一个任务，并能随着更多信息来逐步调整计划。但是，这里还需要人工的操作，需要人工通过自然语言给出提示词，并进行管控。人类聚焦在目标上，agent聚焦在找到完成目标需要执行的细节上。当人机交互界面发生变化时，AgentAI系统仍然可以依赖于相同的后端来完成任务。以买玩具汽车为例，Agentic AI系统能够调用购买、库存更新、物流以及订单确认的工作流，在这些工作流中需要调用相同的后端服务和数据库。但问题是：Agentic AI系统是否能够直接和数据库来交互来获取库存信息或者调用信用卡网络API来付费？也就是：Agentic AI系统通过什么方式与数据交互？

在这篇博客中，作者探讨了直接数据库访问来替代智能体AI服务的成本、收益和缺点，包括在生产中验证过的有效的方式，以及还需要构建的服务。

**Agentic AI 系统的架构**

Agentic AI应用的核心是一个循环（loop）。当用户让系统完成一项任务时，工作流就进入了一个event loop，在这个loop中会不断迭代，直至认为完成了任务或回答了用户的问题。系统可能会回到用户那里寻求更多信息。这个loop实际上就是Reason+Act（ReAct）loop，是Agentic AI系统最流行的设计模式。

![Agentic AI设计Loop](https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2025/10/02/DBBLOG-5187-2.png)


ReAct模式被广泛应用在各式Agent中，比如Chatbot，商业流程自动化Agent、研究类Agent等。实现过程中，可以利用**Agent开发框架**如StrandsAgent、LangGraph，**托管服务**如Amazon Bedrock AgentCore，以及**用户Agent应用**如Amazon Q Business、Q Developer、Claude Desktop等。实现这类Agent，通常需要考虑如下模块：

1.上下文管理模块。负责为每一轮循环检索、汇聚和过滤所需数据。数据来源包括会话状态、历史记忆（如会话历史、用户偏好等）以及工具执行结果。检索数据后，该模块会选择最相关的信息放入下一轮LLM调用的上下文窗口中。

2.推理和规划模块。负责理解用户意图，结合相关上下文来制定或调整行动计划。该模块会判断当前信息是否足够回答用户问题，如果不够就继续执行循环，将每次的结果保存到记忆中，直到完成用户任务。

3.工具/Action执行模块。使用可用工具来执行完成用户任务的具体行动。该模块接收推理和规划模块提供的参数作为工具调用的输入，然后将执行结果传递给上下文管理模块，用于下一轮循环。

这个架构使得Agentic AI应用能够在设计范围内，通过多轮Event Loop来完成指定的任务。在执行任务过程中，Event Loop循环次数越多，系统效率越低。由于LLM调用次数不可预测，会带来不确定的输入/输出token消耗，进而影响成本和性能。从Event Loop中跳出来向用户获取更多数据同样也会影响成本和性能。接下来我们来看有哪些机制可以优化这个loop和用户体验。


**Agentic AI应用的上下文管理模块**

尽管AI Agent获得了大模型的复杂推理能力，但它们面临着这些模型固有的两个关键限制。首先，大模型是无状态的，无法记住过去的交互或维护上下文。其次，大模型有限的上下文窗口限制了每一步能够处理的数据量。

上下文管理模块帮助开发者管理Agent每次Event Loop迭代的上下文窗口。上下文工程是指在Agent执行的每一步中仔细选择要包含在上下文窗口中的正确信息的艺术和科学。这些信息可能包括用户指令（系统和用户提示词、工具描述等）、先前的记忆（会话历史、用户偏好等）、外部知识（企业知识库、权威数据源等）、工具执行结果（API调用结果）以及其他数据源。

当Agent执行循环时，可能会从这些数据源积累大量信息，这可能导致上下文超出LLM的上下文窗口，因此聚合先前交互的数据并不是有效的解决方案。即使对于支持上下文窗口中大量token的LLM模型，推理的有效性也会随着上下文量的增加而衰减，使Agent更容易产生幻觉、忽略有价值的信息或无法执行任务。因此，仔细过滤和选择最有价值的数据包含在每次LLM推理的上下文窗口中至关重要。

Agent访问的数据源包括：

*指令*

LLM交互的指令包括系统提示词、用户提示词、工具选择信息以及结果示例。系统提示词为Agent提供角色、能力、个性和约束等指令。用户提示词是用户的直接请求。工具选择信息是Agent选择正确工具执行任务所需的上下文数据。

*短期记忆*

短期记忆包括会话（Session）和状态（State）。Session存储对话和任务执行中交互的时间顺序流。类似于Web应用的session，会话数据需要快速访问近期交互历史，采用临时存储方式，并设置基于时间的自动过期机制。State相当于Agent为每个特定交互准备的专用记事本。Agent使用State存储空间来存放和更新任务执行期间所需的动态详细信息，如待办事项列表及其执行进度。在考虑存储此类数据的合适基础设施时，需要考虑高频更新、可配置的保留策略、会话结束时的内存清理以及细粒度的会话级访问控制。像Q Developer、Claude Code和Cline这样的本地Agent通常使用文件系统存储Session和State信息。在线Agent可以使用专门的短期记忆服务，如Bedrock AgentCore短期记忆或Amazon ElastiCache for Valkey等缓存服务。（注：Amazon DynamoDB/DocumentDB/Aurora PostgreSQL也都是可以考虑的选项）

*长期记忆*

长期记忆包括Agent随时间积累的事实、关系以及学习到的行为模式。有了长期记忆，Agent在与每个客户交互或解决问题时不需要从头开始。记忆使Agent能够利用先前的成功模式、学到的上下文以及对客户的了解，通过更少的客户交互轮次更快地解决问题。

在Agent收集了大量信息后（通常在会话结束时），它可以执行并行的记忆提取过程。该过程涉及将Session和State数据传递给LLM来识别相关信息。新提取的记忆可以存储在向量数据库中以供后续相似性搜索，或存储在关系数据库中以支持更确定性的查询。（注：客户可以自己配置什么时候来触发长期记忆的提取，另外也可以调用一些框架来进行信息抽取，比如Mem0）

存储长期记忆对数据库的技术需求包括向量搜索和预过滤能力，以缩小搜索范围并改善查找延迟。Bedrock AgentCore的记忆原语提供了这些功能，以及自定义的分块和摘要策略。开发者也可以选择Aurora PostgreSQL或OpenSearch Service将向量与应用数据保持在一起。

为了改善记忆检索，开发者可以将向量搜索与知识图谱结合起来（GraphRAG）。开发者可以在知识图谱中将记忆组织为相互连接的实体和关系，使Agent能够理解复杂的依赖关系并基于连接的信息进行推理。Amazon Neptune Analytics提供了图定义、检索和动态关系创建支持，用于从知识图谱中存储和检索记忆。（注：AI Coding可以将代码的调用关系等存储到知识图谱中，来进行检索）

*外部知识库*

知识库是结构化和非结构化信息的集中存储库，作为AI应用的基础数据。它们包括文档、FAQ、产品规格、程序、最佳实践和特定领域的专业知识。数据库如Aurora PostgreSQL将文本存储为高维向量（Embedding），支持理解上下文和含义而不仅仅是关键词匹配的向量搜索语义级别的向量检索。这些功能对于Agent使用向量搜索检索最相关信息以识别语义相似性至关重要。开发者还可以使用专门的RAG服务如Amazon Bedrock Knowledge Bases创建知识库，该服务提供端到端的RAG管理工作流。

*从工具执行过程中检索到的数据信息*

工具充当AI Agent和外部系统之间的桥梁，实现动态数据访问。Agent可以通过工具检索各种数据类型，如实时信息（天气、股价、新闻）、计算结果（计算、数据分析）以及外部API数据（CRM记录、库存水平）。有了适当的上下文，推理和规划组件可以确定完成用户任务的最有效路径。

**Agentic AI应用的推理和规划模块**

推理和规划组件收集最终用户问题和来自先前event loop迭代的所有上下文，并将其作为输入提交给LLM进行推理。发送给LLM的上下文包括经过上下文管理模块过滤的最关键数据，包括指令、记忆、知识库中的数据，以及数据库相关的工具。从LLM接收的响应可以是三个选项之一：

*回应用户*

LLM推理输出可以包含对用户的响应，表明任务已完成（或在极少数情况下，Agent因错误或其他原因放弃）。

*向用户询问更多信息*

与回应用户类似，此选项将期望用户的回应。有了用户在上下文中的额外数据，系统可以开始新的event loop迭代。

*选择要调用的工具*
LLM推理输出可以包含指定要调用哪个工具（工具选项作为系统提示的一部分提供）和调用该工具要使用的输入参数的指令。

Agentic AI系统通常比我们这里描述的更复杂，因为它们需要最小化完成任务所需的event loop迭代次数，并在LLM调用方面更加谨慎，以实现最佳性价比。高级推理和规划组件可能使用规则或LLM将用户问题分解为完成任务所需的一系列行动（步骤）。某些计划的行动可能并行执行（如果它们不相互依赖），从而减少延迟（例如，Strands允许LLM使用批处理接口并行调用工具）。对于其他行动，确定行动计划的初始LLM响应可能已经提供了所需的输入参数，从而避免在工具调用之前进行另一次LLM调用。如果需要单独的LLM调用来推导工具输入参数，可能可以使用更小的专用LLM或小型语言模型（SLM）。这些优化改善了延迟并降低了推理成本。

一种新兴的设计模式将一些次要的LLM调用转移到工具本身。例如，设计用于在关系数据库上执行查询的工具可能接受自然语言，然后自己调用LLM生成相应的SQL查询，然后连接到数据库并执行查询。该工具可以使用针对生成SQL查询优化的模型，而不是更大的通用模型。这样的工具实际上本身就是一个Agent，有自己的event loop，迭代尝试生成的查询，直到获得正确的查询。在这种将Agent作为工具的模式中，主Agent的记忆不会被工具中失败查询尝试的上下文污染。（参考：[开放协议促进Agent互操作性：MCP上的Agent间通信](https://aws.amazon.com/blogs/opensource/open-protocols-for-agent-interoperability-part-1-inter-agent-communication-on-mcp/)）

**工具调用模块**

虽然可能很容易认为推理和规划组件中的LLM直接调用正确的工具，但事实并非如此。LLM没有直接采取行动或执行代码的能力；相反，它返回一组指令，指定要运行什么工具以及要使用什么参数。Agentic AI系统会在接下来的event loop中解析这个指令，来进行工具调用并传递相应参数，根据工具的预期工作方式返回自然语言或结构化响应。

什么算作工具？任何接受一组输入参数并返回响应的业务逻辑都算作工具。比如像StrandsAgent这样的框架，就提供了一个内嵌的返回当前日期和时间的工具。这是一个基本工具，在时间点对上下文很重要的情况下可能很有用。在另一个极端，你可以将整个Agent作为工具暴露给其他Agent，如前所述。有了如此多样的工具，你需要一种标准化的方式来访问它们。

MCP（Model Context Protocol）提供了在Agentic AI系统中声明和使用工具的标准化方式。该协议有服务器和客户端组件。MCP服务器公开一组可以通过MCP协议访问的工具。例如，Amazon Aurora DSQL MCP服务器提供三个工具：表模式发现、只读查询和改变数据的事务。使用这三个工具，Agentic AI系统可以检索数据库的模式，组成SQL查询，执行它们并返回结果。

MCP客户端是你的Agentic AI系统中的一个接口组件。因为MCP协议的标准化，MCP客户端可以与任何MCP服务器通信。通过在你的Agentic AI系统中实现MCP客户端，你可以注册MCP服务器提供的任何工具，并让你的Agent开始使用它。下图显示了使用五个工具的示例AI Agent。一个工具是内置的，三个工具可通过MCP访问，最后一个是作为工具公开的单独Agent。

![AI Agent工具使用示例](https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2025/10/02/DBBLOG-5187-3.png)

所有工具都以某种形式对数据进行操作。为了更好地突出数据在智能体AI系统中的作用，我们将工具分为两类：数据检索工具和数据操作工具。

数据检索工具用于获取Agent完成任务所需的相关上下文。这些数据直接从数据库、数据湖和分析系统检索。或者，数据也可以从其他服务和应用程序（如企业资源规划（ERP）系统）检索。除了从向量数据库进行语义数据检索外，智能体AI对数据库技术没有特定要求或偏好。数据库技术应该主要基于存储数据的性质和访问模式来选择。Agentic AI更倾向于让用户能够访问多样化的数据服务，而不是依赖特定服务。AWS为绝大多数的数据库和分析服务都提供了MCP Server，比如Aurora，DynamoDB，ElastiCache，Redshift和S3 Table等。这些MCP简化了AgentiC AI应用和这些服务中存储数据的交互过程。

数据操作工具用于对数据进行更改，无论是直接在数据库中还是通过其他服务和API间接进行。就像先前提到的使用Agentic AI 买玩具消防车的例子，Agentic AI系统会调用工具来减少库存、处理交易信息，并将订单加入到订单历史中。这些行动涉及对各种数据存储中的数据进行更改，即使它们使用API进行抽象。。

上面的例子又带来工具之间的另一个区分：通用工具还是专用工具。使用一个例子来进行说明：作者之一Vlad是乐高玩具的忠实粉丝，喜欢为他拥有的所有套装（数量太多了）建立积木清单。这帮助他识别缺失的积木，或者了解他拥有多少特定类型的积木。他把这些数据存储在Aurora DSQL中，因为Aurora DSQL是一个弹性的关系型数据库，而且运维成本极低。

Vlad可以使用Aurora DSQL MCP服务器并将其插入智能体AI系统，如Amazon Q或Claude。现在，这些应用程序可以回答关于他的乐高收藏的问题，因为它们有适当的工具来做到这一点。这设置起来快速高效，他可以根据需要询问各种各样的问题。系统内部发生了什么？Agent经过多次事件循环迭代才能得到答案：首先它必须发现数据库的模式，然后它必须组装正确的SQL查询来回答Vlad的问题。

根据经验，通常需要额外的两到三次事件循环迭代才能生成获取他所需数据的有效SQL查询。这使用了大约四次LLM API调用来获得答案。即使需要很长时间才能得到答案，相对于Vlad设置这个通用MCP服务器所做的努力水平，如果他不经常询问这些问题，这是可以接受的。

但是，如果Vlad经常询问他的Agent关于拥有的特定积木的库存水平怎么办？从扩展性角度考虑，为他构建一个专门的工具来确定性地检索他的积木库存水平将更有效。这样，他可以有效地消除执行文本到SQL转换的LLM调用，将LLM API调用使用量减少到可能只有一次：推理和规划组件确定需要使用积木类型和颜色作为输入参数调用他的专用工具。这通常被称为预设查询MCP模式。通用工具和专用工具通常是互补的。在这个用例中，他可以将专用工具用于频繁查询，将通用工具用于一次性查询。

你可以看到，减少完成任务所需的事件循环迭代次数将是智能体AI应用程序性能和成本优化的关键，而数据的使用是实现这一目标的一种方式。

虽然通用工具和专用工具是互补的，但它们适合不同的角色。对于事务系统，将数据库交互包装到服务中并将这些服务作为工具公开会导致更确定性的结果和更好的整体性能。相比之下，与数据库交互的通用工具（text2SQL或类似工具）在
分析系统和数据发现用例中更有用。

为了完整性，让我们补充event loop后面的内容。调用工具后，结果被反馈到智能体AI系统的记忆中，以便上下文管理可以应用于它们。e=Event loop的下一次迭代将在下一个决策的上下文中包含这些结果。

在考虑更高级的实现时，可以将其他组件添加到event loop中，例如guard rail组件以保持Agent响应正确、专注和伦理；或学习和适应组件，使用反馈循环从过去的经验中学习。

**总结**
智能体AI系统的模块化特性意味着你可以随时间添加更多功能，无论你是在构建智能体AI应用程序还是使用现有的智能体AI应用程序，如Amazon Q或Claude。你可以使用MCP服务器扩展这些系统的能力。使用这种机制来访问你的数据，无论是操作数据还是检索上下文，都能让这些应用程序更好地适应你的特定用例。理解数据如何与你的应用程序交互（通过记忆和工具），使你能够更好地选择正确的数据架构，同时考虑数据的性质和访问方式。
